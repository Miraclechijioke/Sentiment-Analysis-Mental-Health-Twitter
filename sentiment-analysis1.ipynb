{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: nltk in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (3.9.1)\nRequirement already satisfied: click in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk) (2024.9.11)\nRequirement already satisfied: tqdm in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk) (4.66.4)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1726743148447
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Ensure nltk resources are available\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize the lemmatizer and stop words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Custom stop words list with negations retained\n",
        "custom_stop_words = stop_words - {'not', 'no', 'but', 'because'}\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Convert text to Lowercase\n",
        "    text = text.lower()\n",
        "    # Removing special characters and numbers\n",
        "    text = re.sub(r'[^\\w\\s,.]', '', text)  # Remove punctuation, Keep periods and commas for sentence flow\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    # Tokenization\n",
        "    tokens = text.split()\n",
        "    # Removing stop words and stemming\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in custom_stop_words]\n",
        "    # Rejoin tokens into a single string\n",
        "    text = ' '.join(tokens)\n",
        "    return text"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2024-09-19 10:52:41.992240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-19 10:52:42.727670: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-19 10:52:42.949485: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-09-19 10:52:44.606999: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-09-19 10:52:47.918466: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726743171551
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Mental-Health-Twitter.csv', index_col=0)\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726743171937
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "(20000, 10)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726743172290
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "              post_id                    post_created  \\\n0  637894677824413696  Sun Aug 30 07:48:37 +0000 2015   \n1  637890384576778240  Sun Aug 30 07:31:33 +0000 2015   \n2  637749345908051968  Sat Aug 29 22:11:07 +0000 2015   \n3  637696421077123073  Sat Aug 29 18:40:49 +0000 2015   \n4  637696327485366272  Sat Aug 29 18:40:26 +0000 2015   \n\n                                           post_text     user_id  followers  \\\n0  It's just over 2 years since I was diagnosed w...  1013187241         84   \n1  It's Sunday, I need a break, so I'm planning t...  1013187241         84   \n2  Awake but tired. I need to sleep but my brain ...  1013187241         84   \n3  RT @SewHQ: #Retro bears make perfect gifts and...  1013187241         84   \n4  It’s hard to say whether packing lists are mak...  1013187241         84   \n\n   friends  favourites  statuses  retweets  label  \n0      211         251       837         0      1  \n1      211         251       837         1      1  \n2      211         251       837         0      1  \n3      211         251       837         2      1  \n4      211         251       837         1      1  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>post_id</th>\n      <th>post_created</th>\n      <th>post_text</th>\n      <th>user_id</th>\n      <th>followers</th>\n      <th>friends</th>\n      <th>favourites</th>\n      <th>statuses</th>\n      <th>retweets</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>637894677824413696</td>\n      <td>Sun Aug 30 07:48:37 +0000 2015</td>\n      <td>It's just over 2 years since I was diagnosed w...</td>\n      <td>1013187241</td>\n      <td>84</td>\n      <td>211</td>\n      <td>251</td>\n      <td>837</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>637890384576778240</td>\n      <td>Sun Aug 30 07:31:33 +0000 2015</td>\n      <td>It's Sunday, I need a break, so I'm planning t...</td>\n      <td>1013187241</td>\n      <td>84</td>\n      <td>211</td>\n      <td>251</td>\n      <td>837</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>637749345908051968</td>\n      <td>Sat Aug 29 22:11:07 +0000 2015</td>\n      <td>Awake but tired. I need to sleep but my brain ...</td>\n      <td>1013187241</td>\n      <td>84</td>\n      <td>211</td>\n      <td>251</td>\n      <td>837</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>637696421077123073</td>\n      <td>Sat Aug 29 18:40:49 +0000 2015</td>\n      <td>RT @SewHQ: #Retro bears make perfect gifts and...</td>\n      <td>1013187241</td>\n      <td>84</td>\n      <td>211</td>\n      <td>251</td>\n      <td>837</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>637696327485366272</td>\n      <td>Sat Aug 29 18:40:26 +0000 2015</td>\n      <td>It’s hard to say whether packing lists are mak...</td>\n      <td>1013187241</td>\n      <td>84</td>\n      <td>211</td>\n      <td>251</td>\n      <td>837</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726743172596
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('Mental-Health-Twitter.csv', index_col=0)  # Replace with your dataset path\n",
        "texts = data['post_text'].apply(preprocess_text)  # Apply preprocessing\n",
        "labels = data['label']  # Replace with your label column name\n",
        "\n",
        "labels = np.array(labels)\n",
        "# labels = to_categorical(labels, num_classes=2)\n",
        "\n",
        "# Apply preprocessing\n",
        "# processed_texts = texts.apply(preprocess_text)\n",
        "\n",
        "# Calculate the length of each processed text\n",
        "lengths = texts.apply(lambda x: len(x.split()))\n",
        "\n",
        "# Calculate the average length\n",
        "average_length = round(lengths.mean())\n",
        "\n",
        "print(f\"Average length of reviews: {average_length}\")\n",
        "\n",
        "# Split the dataset\n",
        "x_train, x_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "\n",
        "# Tokenization and Padding\n",
        "max_features = 10000  # Number of unique words to keep\n",
        "maxlen = average_length * 2  # Maximum length of sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "x_train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_sequences = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "x_train_padded = pad_sequences(x_train_sequences, maxlen=maxlen)\n",
        "x_test_padded = pad_sequences(x_test_sequences, maxlen=maxlen)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Average length of reviews: 8\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726743176482
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "10227    rt bonmotvivant lili reinhart say betty veroni...\n6597     nativeandnaive legendxofxzach one time freshma...\n6355     zaynmalik dont forget belong zayn, heart milli...\n14190                         battlafield baba_khan uh huh\n8059     rt historyinpics mcdonalds menu early httpt.co...\n                               ...                        \n335      kid screen electronics addiction httpst.corwcx...\n4235     gradingus would answered yes say specify sausa...\n1617     toda adventure natural option depression easie...\n1815     annie testing help sufferer depressive disorde...\n13728                                          friend dead\nName: post_text, Length: 14000, dtype: object"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726743176784
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',    # Watch the validation loss\n",
        "    patience=2,            # Stop after 3 epochs of no improvement\n",
        "    restore_best_weights=True  # Revert to the best model\n",
        ")\n",
        "\n",
        "# Build the LSTM model\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(input_dim=max_features, output_dim=128),\n",
        "    layers.LSTM(64, return_sequences=True),\n",
        "    layers.LSTM(32),\n",
        "    layers.Dense(1, activation='sigmoid') #It outputs a value between 0 and 1, representing the probability that the input belongs to the positive class.\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train_padded, y_train, epochs=8, validation_data=(x_test_padded, y_test), callbacks=[early_stopping])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/8\n\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - accuracy: 0.7382 - loss: 0.4966 - precision: 0.7789 - recall: 0.6453 - val_accuracy: 0.8615 - val_loss: 0.2961 - val_precision: 0.9085 - val_recall: 0.8040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2/8\n\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - accuracy: 0.9284 - loss: 0.1651 - precision: 0.9292 - recall: 0.9281 - val_accuracy: 0.8727 - val_loss: 0.2763 - val_precision: 0.8658 - val_recall: 0.8820\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3/8\n\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.9675 - loss: 0.0866 - precision: 0.9679 - recall: 0.9672 - val_accuracy: 0.8727 - val_loss: 0.4151 - val_precision: 0.8734 - val_recall: 0.8717\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 4/8\n\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.9786 - loss: 0.0566 - precision: 0.9780 - recall: 0.9793 - val_accuracy: 0.8693 - val_loss: 0.4330 - val_precision: 0.8736 - val_recall: 0.8637\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2024-09-19 10:53:02.720651: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726743225462
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model and unpack all values\n",
        "results = model.evaluate(x_test_padded, y_test, verbose=2)\n",
        "test_loss = results[0]\n",
        "test_acc = results[1]\n",
        "test_precision = results[2]\n",
        "test_recall = results[3]\n",
        "\n",
        "print(f\"Test loss: {test_loss}\")\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "print(f\"Test precision: {test_precision}\")\n",
        "print(f\"Test recall: {test_recall}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "188/188 - 1s - 3ms/step - accuracy: 0.8727 - loss: 0.2763 - precision: 0.8658 - recall: 0.8820\nTest loss: 0.2763007879257202\nTest accuracy: 0.8726666569709778\nTest precision: 0.8658376932144165\nTest recall: 0.8820000290870667\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726743226087
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Get predictions for classification metrics\n",
        "y_pred = (model.predict(x_test_padded) > 0.5).astype(\"int32\")  # Convert probabilities to 0 or 1\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.86      0.87      3000\n           1       0.87      0.88      0.87      3000\n\n    accuracy                           0.87      6000\n   macro avg       0.87      0.87      0.87      6000\nweighted avg       0.87      0.87      0.87      6000\n\nConfusion Matrix:\n[[2590  410]\n [ 354 2646]]\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726743227362
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}