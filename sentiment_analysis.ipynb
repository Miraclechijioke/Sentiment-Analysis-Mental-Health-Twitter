{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting nltk\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: click in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk) (1.4.2)\nCollecting regex>=2021.8.3 (from nltk)\n  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk) (4.66.4)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: regex, nltk\nSuccessfully installed nltk-3.9.1 regex-2024.9.11\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726590970317
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Ensure nltk resources are available\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize the lemmatizer and stop words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Custom stop words list with negations retained\n",
        "custom_stop_words = stop_words - {'not', 'no', 'but', 'because'}\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Convert text to Lowercase\n",
        "    text = text.lower()\n",
        "    # Removing special characters and numbers\n",
        "    text = re.sub(r'[^\\w\\s,.]', '', text)  # Remove punctuation, Keep periods and commas for sentence flow\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    # Tokenization\n",
        "    tokens = text.split()\n",
        "    # Removing stop words and stemming\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in custom_stop_words]\n",
        "    # Rejoin tokens into a single string\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2024-09-17 18:22:34.770288: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-17 18:22:35.470960: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-17 18:22:35.679895: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-09-17 18:22:37.243809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-09-17 18:22:40.216460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1726597367423
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''While preparing for my Microsoft Data Science Associate certification, I am also interning as a Generative AI/Data Science at Flexisaf Edusoft Limited.\n",
        "\n",
        "So, there is a project I am tasked to work on and deliver in 2 weeks.\n",
        "\n",
        "I am currently working on a sentiment analysis project. Sentiment analysis is a natural language processing task where the goal is to determine the sentiment expressed in text, such as positive, negative, or neutral. In this project, I made use of a dataset(from Kaggle) of text data labeled with sentiment classes and I am to build a deep learning model to classify sentiment automatically.\n",
        "\n",
        "Step 1: Data Preprocessing\n",
        "Before feeding the text into a deep learning model, it’s essential to clean and preprocess it. Text data can be messy and inconsistent, and deep learning models work best with well-structured input.\n",
        "\n",
        "Lowercasing: The first step was converting all text to lowercase. This was done to ensure uniformity, as “Happy” and “happy” would otherwise be treated as different words by the model.\n",
        "\n",
        "Removing Special Characters and Numbers: I removed punctuation marks and numbers, as these typically don’t carry much value in sentiment analysis and could introduce noise.\n",
        "\n",
        "Tokenization: I split the text into individual words or tokens. This step is essential for breaking the text down into manageable parts.\n",
        "\n",
        "Stop Words Removal: Next, I removed common words like “the,” “is,” and “on” using a predefined set of stop words. These words do not contribute significant meaning to sentiment and can be safely ignored.\n",
        "\n",
        "Stemming: I used stemming to reduce words to their base or root form. For example, “running” becomes “run.” This helps group different forms of a word as the same entity, reducing vocabulary size.\n",
        "\n",
        "\n",
        "Step 2: Splitting the Dataset\n",
        "After preprocessing, I divided the dataset into training and test sets. The training set was used to train the model, while the test set was kept aside to evaluate the model’s performance later(A norm when training models to perform tasks)\n",
        "\n",
        "Step 3: Tokenization and Padding\n",
        "Next, I needed to convert the text into a numerical form that the deep learning model could understand.\n",
        "\n",
        "Tokenization: I used a tokenizer to convert words into numerical sequences. The tokenizer assigns a unique integer to each word, allowing the model to process the text as a sequence of numbers.\n",
        "\n",
        "Padding: Since the input text can vary in length, I applied padding to ensure that all input sequences have the same length. This consistency is necessary for feeding data into a neural network.\n",
        "\n",
        "Training Data: 70% of the data was used for training.\n",
        "Test Data: 30% of the data was used for testing and validation.\n",
        "\n",
        "Step 4: Building the Deep Learning Model\n",
        "For this task, I used a recurrent neural network (RNN), which is effective for sequence data like text. Specifically, I implemented an LSTM (Long Short-Term Memory) network, a type of RNN that is well-suited to capturing long-term dependencies in text.\n",
        "\n",
        "The model consists of an embedding layer to convert word sequences into dense vector representations.\n",
        "Followed by LSTM layers, which process the sequence data and learn the context of the text.\n",
        "Finally, a dense output layer with a sigmoid activation function to classify the sentiment as positive or negative.\n",
        "\n",
        "Step 5: Training and Evaluation\n",
        "After setting up the model, I trained it using the training data. I monitored key metrics during training, such as:\n",
        "\n",
        "Accuracy: How often the model predicts the correct sentiment.\n",
        "Precision and Recall: To ensure the model balances positive and negative predictions well.\n",
        "Once trained, I evaluated the model on the test set to see how well it performs on unseen data. The results gave insight into the model’s generalization capabilities.\n",
        "\n",
        "Conclusion:\n",
        "In this project, I successfully built and trained a deep learning model to perform sentiment analysis on a custom dataset. The preprocessing pipeline ensured clean and consistent data, and the LSTM model provided robust performance in capturing the context of the text to classify sentiments accurately. This model can now be applied to various applications, such as customer reviews, social media monitoring, and more.\"'''"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726592289550
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text(text)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": "'preparing microsoft data science associate certification, also interning generative aidata science flexisaf edusoft limited. so, project tasked work deliver weeks. currently working sentiment analysis project. sentiment analysis natural language processing task goal determine sentiment expressed text, positive, negative, neutral. project, made use datasetfrom kaggle text data labeled sentiment class build deep learning model classify sentiment automatically. step data preprocessing feeding text deep learning model, essential clean preprocess it. text data messy inconsistent, deep learning model work best wellstructured input. lowercasing first step converting text lowercase. done ensure uniformity, happy happy would otherwise treated different word model. removing special character number removed punctuation mark numbers, typically dont carry much value sentiment analysis could introduce noise. tokenization split text individual word tokens. step essential breaking text manageable parts. stop word removal next, removed common word like the, is, using predefined set stop words. word not contribute significant meaning sentiment safely ignored. stemming used stemming reduce word base root form. example, running becomes run. help group different form word entity, reducing vocabulary size. step splitting dataset preprocessing, divided dataset training test sets. training set used train model, test set kept aside evaluate model performance latera norm training model perform task step tokenization padding next, needed convert text numerical form deep learning model could understand. tokenization used tokenizer convert word numerical sequences. tokenizer assigns unique integer word, allowing model process text sequence numbers. padding since input text vary length, applied padding ensure input sequence length. consistency necessary feeding data neural network. training data data used training. test data data used testing validation. step building deep learning model task, used recurrent neural network rnn, effective sequence data like text. specifically, implemented lstm long shortterm memory network, type rnn wellsuited capturing longterm dependency text. model consists embedding layer convert word sequence dense vector representations. followed lstm layers, process sequence data learn context text. finally, dense output layer sigmoid activation function classify sentiment positive negative. step training evaluation setting model, trained using training data. monitored key metric training, accuracy often model predicts correct sentiment. precision recall ensure model balance positive negative prediction well. trained, evaluated model test set see well performs unseen data. result gave insight model generalization capabilities. conclusion project, successfully built trained deep learning model perform sentiment analysis custom dataset. preprocessing pipeline ensured clean consistent data, lstm model provided robust performance capturing context text classify sentiment accurately. model applied various applications, customer reviews, social medium monitoring, more.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726592332701
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Mental-Health-Twitter.csv', index_col=0)\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726597345778
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "              post_id                    post_created  \\\n0  637894677824413696  Sun Aug 30 07:48:37 +0000 2015   \n1  637890384576778240  Sun Aug 30 07:31:33 +0000 2015   \n2  637749345908051968  Sat Aug 29 22:11:07 +0000 2015   \n3  637696421077123073  Sat Aug 29 18:40:49 +0000 2015   \n4  637696327485366272  Sat Aug 29 18:40:26 +0000 2015   \n\n                                           post_text     user_id  followers  \\\n0  It's just over 2 years since I was diagnosed w...  1013187241         84   \n1  It's Sunday, I need a break, so I'm planning t...  1013187241         84   \n2  Awake but tired. I need to sleep but my brain ...  1013187241         84   \n3  RT @SewHQ: #Retro bears make perfect gifts and...  1013187241         84   \n4  It’s hard to say whether packing lists are mak...  1013187241         84   \n\n   friends  favourites  statuses  retweets  label  \n0      211         251       837         0      1  \n1      211         251       837         1      1  \n2      211         251       837         0      1  \n3      211         251       837         2      1  \n4      211         251       837         1      1  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>post_id</th>\n      <th>post_created</th>\n      <th>post_text</th>\n      <th>user_id</th>\n      <th>followers</th>\n      <th>friends</th>\n      <th>favourites</th>\n      <th>statuses</th>\n      <th>retweets</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>637894677824413696</td>\n      <td>Sun Aug 30 07:48:37 +0000 2015</td>\n      <td>It's just over 2 years since I was diagnosed w...</td>\n      <td>1013187241</td>\n      <td>84</td>\n      <td>211</td>\n      <td>251</td>\n      <td>837</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>637890384576778240</td>\n      <td>Sun Aug 30 07:31:33 +0000 2015</td>\n      <td>It's Sunday, I need a break, so I'm planning t...</td>\n      <td>1013187241</td>\n      <td>84</td>\n      <td>211</td>\n      <td>251</td>\n      <td>837</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>637749345908051968</td>\n      <td>Sat Aug 29 22:11:07 +0000 2015</td>\n      <td>Awake but tired. I need to sleep but my brain ...</td>\n      <td>1013187241</td>\n      <td>84</td>\n      <td>211</td>\n      <td>251</td>\n      <td>837</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>637696421077123073</td>\n      <td>Sat Aug 29 18:40:49 +0000 2015</td>\n      <td>RT @SewHQ: #Retro bears make perfect gifts and...</td>\n      <td>1013187241</td>\n      <td>84</td>\n      <td>211</td>\n      <td>251</td>\n      <td>837</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>637696327485366272</td>\n      <td>Sat Aug 29 18:40:26 +0000 2015</td>\n      <td>It’s hard to say whether packing lists are mak...</td>\n      <td>1013187241</td>\n      <td>84</td>\n      <td>211</td>\n      <td>251</td>\n      <td>837</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726597346231
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('Mental-Health-Twitter.csv', index_col=0)  # Replace with your dataset path\n",
        "texts = data['post_text'].apply(preprocess_text)  # Apply preprocessing\n",
        "labels = data['label']  # Replace with your label column name\n",
        "\n",
        "# labels = to_categorical(labels, num_classes=2)\n",
        "\n",
        "# Apply preprocessing\n",
        "# processed_texts = texts.apply(preprocess_text)\n",
        "\n",
        "# Calculate the length of each processed text\n",
        "lengths = texts.apply(lambda x: len(x.split()))\n",
        "\n",
        "# Calculate the average length\n",
        "average_length = round(lengths.mean())\n",
        "\n",
        "print(f\"Average length of reviews: {average_length}\")\n",
        "\n",
        "# Split the dataset\n",
        "x_train, x_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "\n",
        "# Tokenization and Padding\n",
        "max_features = 10000  # Number of unique words to keep\n",
        "maxlen = average_length * 2  # Maximum length of sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "x_train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_sequences = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "x_train_padded = pad_sequences(x_train_sequences, maxlen=maxlen)\n",
        "x_test_padded = pad_sequences(x_test_sequences, maxlen=maxlen)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Average length of reviews: 8\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726597963455
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Build the LSTM model\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(max_features, 128),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train_padded, y_train, epochs=5, validation_data=(x_test, y_test))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "\n",
        "# Build the LSTM model\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(input_dim=max_features, output_dim=128, input_lenght=maxlen),\n",
        "    layers.LSTM(64, return_sequences=True),\n",
        "    layers.LTSM(32),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train_padded, y_train, epochs=5, validation_data=(x_test, y_test))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model and unpack all values\n",
        "results = model.evaluate(x_test_padded, y_test, verbose=2)\n",
        "test_loss = results[0]\n",
        "test_acc = results[1]\n",
        "test_precision = results[2]\n",
        "test_recall = results[3]\n",
        "\n",
        "print(f\"Test loss: {test_loss}\")\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "print(f\"Test precision: {test_precision}\")\n",
        "print(f\"Test recall: {test_recall}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Get predictions for classification metrics\n",
        "y_pred = (model.predict(x_test_padded) > 0.5).astype(\"int32\")  # Convert probabilities to 0 or 1\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}